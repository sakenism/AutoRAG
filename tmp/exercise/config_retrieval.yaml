# Retrieval-only configuration
# This config will only run retrieval evaluation and stop there

# Define your Vector DB configurations with multiple embedding models
vectordb:
  - name: nomic_embed
    db_type: chroma
    client_type: persistent
    embedding_model:
      type: ollama
      model_name: nomic-embed-text
    collection_name: nomic_embed
    path: ${PROJECT_DIR}/resources/chroma
    embedding_batch: 20
  
  - name: mxbai_embed
    db_type: chroma
    client_type: persistent
    embedding_model:
      type: ollama
      model_name: mxbai-embed-large
    collection_name: mxbai_embed
    path: ${PROJECT_DIR}/resources/chroma
    embedding_batch: 20
  
  # FlagEmbedding models with direct configuration
  - name: flagembed_large
    db_type: chroma
    client_type: persistent
    embedding_model:
      type: huggingface
      model_name: BAAI/bge-large-en-v1.5
    collection_name: flagembed_large
    path: ${PROJECT_DIR}/resources/chroma
    embedding_batch: 20
  
  - name: flagembed_small
    db_type: chroma
    client_type: persistent
    embedding_model:
      type: huggingface
      model_name: BAAI/bge-small-en-v1.5
    collection_name: flagembed_small
    path: ${PROJECT_DIR}/resources/chroma
    embedding_batch: 20

# Chunking configuration with multiple methods to test
chunking:
  modules:
    # First chunking method: Token-based with different sizes
    - module_type: llama_index_chunk
      chunk_method: [Token]
      chunk_size: [256, 512]
      chunk_overlap: [20, 50]
      add_file_name: en
    
    # Second chunking method: Sentence-based
    # - module_type: llama_index_chunk
    #   chunk_method: [Sentence]
    #   chunk_size: [3, 5, 8]  # Number of sentences per chunk
    #   chunk_overlap: [1, 2]  # Overlap in sentences
    #   add_file_name: en
    
    # # Third chunking method: Custom sentence window with custom splitter
    # - module_type: llama_index_chunk
    #   chunk_method: [SentenceWindow]
    #   sentence_splitter: custom_splitter
    #   window_size: [2, 3, 5]
    #   add_file_name: en

# RAG pipeline configuration - RETRIEVAL ONLY
node_lines:
  # Retrieval phase with different options
  - node_line_name: retrieve_line
    nodes:
      - node_type: retrieval
        strategy:
          # Only using non-semantic metrics that don't require OpenAI
          metrics: [retrieval_precision, retrieval_recall, retrieval_f1]
          speed_threshold: 60
        top_k: [2, 3]  # Reduced from [3, 5] to decrease context size
        modules:
          - module_type: vectordb
            vectordb: nomic_embed
          - module_type: vectordb
            vectordb: mxbai_embed
          - module_type: vectordb
            vectordb: flagembed_large
          - module_type: vectordb
            vectordb: flagembed_small
          - module_type: bm25
          - module_type: hybrid_rrf  # Reciprocal Rank Fusion
            rrf_k: [1, 3]  # Reduced from [1, 3, 5]

