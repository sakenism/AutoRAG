# Define your Vector DB configurations with multiple embedding models
vectordb:
  - name: nomic_embed
    db_type: chroma
    client_type: persistent
    embedding_model:
      type: ollama
      model_name: nomic-embed-text
    collection_name: nomic_embed
    path: ${PROJECT_DIR}/resources/chroma
    embedding_batch: 20
  
  - name: mxbai_embed
    db_type: chroma
    client_type: persistent
    embedding_model:
      type: ollama
      model_name: mxbai-embed-large
    collection_name: mxbai_embed
    path: ${PROJECT_DIR}/resources/chroma
    embedding_batch: 20
  
  # FlagEmbedding models with direct configuration
  - name: flagembed_large
    db_type: chroma
    client_type: persistent
    embedding_model:
      type: huggingface
      model_name: BAAI/bge-large-en-v1.5
    collection_name: flagembed_large
    path: ${PROJECT_DIR}/resources/chroma
    embedding_batch: 20
  
  - name: flagembed_small
    db_type: chroma
    client_type: persistent
    embedding_model:
      type: huggingface
      model_name: BAAI/bge-small-en-v1.5
    collection_name: flagembed_small
    path: ${PROJECT_DIR}/resources/chroma
    embedding_batch: 20

# Chunking configuration with multiple methods to test
chunking:
  modules:
    # First chunking method: Token-based with different sizes
    - module_type: llama_index_chunk
      chunk_method: [Token]
      chunk_size: [256, 512, 1024]
      chunk_overlap: [20, 50]
      add_file_name: en
    
    # Second chunking method: Sentence-based
    - module_type: llama_index_chunk
      chunk_method: [Sentence]
      chunk_size: [3, 5, 8]  # Number of sentences per chunk
      chunk_overlap: [1, 2]  # Overlap in sentences
      add_file_name: en
    
    # Third chunking method: Custom sentence window with custom splitter
    - module_type: llama_index_chunk
      chunk_method: [SentenceWindow]
      sentence_splitter: custom_splitter
      window_size: [2, 3, 5]
      add_file_name: en

# RAG pipeline configuration for grid search
node_lines:
  # Retrieval phase with different options
  - node_line_name: retrieve_line
    nodes:
      - node_type: retrieval
        strategy:
          # Removed retrieval_hit_rate, using only commonly supported metrics
          metrics: [retrieval_precision, retrieval_recall, retrieval_f1]
          speed_threshold: 60
        top_k: [3, 5, 10]  # Testing different numbers of retrieved chunks
        modules:
          - module_type: vectordb
            vectordb: nomic_embed
          - module_type: vectordb
            vectordb: mxbai_embed
          - module_type: vectordb
            vectordb: flagembed_large
          - module_type: vectordb
            vectordb: flagembed_small
          - module_type: bm25
          - module_type: hybrid_rrf  # Reciprocal Rank Fusion
            rrf_k: [1, 3, 5]
  
  # Generation phase with multiple LLM models
  - node_line_name: generate_line
    nodes:
      # Prompt formatting
      - node_type: prompt_maker
        strategy:
          # Using standard rouge metric
          metrics:
            - metric_name: rouge
          speed_threshold: 30
        modules:
          - module_type: fstring
            prompt: 
              - "Answer to given questions with the following passage: {retrieved_contents} \n\n Question: {query} \n\n Answer:"
              - "Context information: {retrieved_contents}\n\nBased on the context, answer this question: {query}"
              - "You are a helpful assistant. Use the following information to answer the user's question.\nInformation: {retrieved_contents}\n\nUser question: {query}\n\nAnswer:"
      
      # Generation with different models
      - node_type: generator
        strategy:
          # Using standard rouge metric
          metrics:
            - metric_name: rouge
          speed_threshold: 120
        modules:
          # Llama 3 model configuration
          - module_type: llama_index_llm
            llm: openailike
            model: llama3:latest
            api_base: "http://localhost:11434/v1"
            api_key: "ollama-not-needed-but-required"
            temperature: [0.0, 0.3, 0.7]
            max_tokens: 1500
            request_timeout: 180
            timeout: 180
          
          # Mistral model configuration
          - module_type: llama_index_llm
            llm: openailike
            model: mistral:latest
            api_base: "http://localhost:11434/v1"
            api_key: "ollama-not-needed-but-required"
            temperature: [0.0, 0.3, 0.7]
            max_tokens: 1500
            request_timeout: 180
            timeout: 180