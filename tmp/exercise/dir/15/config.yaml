# Define your Vector DB configurations with multiple embedding models
vectordb:
  - name: nomic_embed
    db_type: chroma
    client_type: persistent
    embedding_model:
      type: ollama
      model_name: nomic-embed-text
    collection_name: nomic_embed
    path: ${PROJECT_DIR}/resources/chroma
    embedding_batch: 20
  
  - name: mxbai_embed
    db_type: chroma
    client_type: persistent
    embedding_model:
      type: ollama
      model_name: mxbai-embed-large
    collection_name: mxbai_embed
    path: ${PROJECT_DIR}/resources/chroma
    embedding_batch: 20
  
  # FlagEmbedding models with direct configuration
  - name: flagembed_large
    db_type: chroma
    client_type: persistent
    embedding_model:
      type: huggingface
      model_name: BAAI/bge-large-en-v1.5
    collection_name: flagembed_large
    path: ${PROJECT_DIR}/resources/chroma
    embedding_batch: 20
  
  - name: flagembed_small
    db_type: chroma
    client_type: persistent
    embedding_model:
      type: huggingface
      model_name: BAAI/bge-small-en-v1.5
    collection_name: flagembed_small
    path: ${PROJECT_DIR}/resources/chroma
    embedding_batch: 20

# Chunking configuration with multiple methods to test
chunking:
  modules:
    # First chunking method: Token-based with different sizes
    - module_type: llama_index_chunk
      chunk_method: [Token]
      chunk_size: [256, 512]
      chunk_overlap: [20, 50]
      add_file_name: en
    
    # Second chunking method: Sentence-based
    # - module_type: llama_index_chunk
    #   chunk_method: [Sentence]
    #   chunk_size: [3, 5, 8]  # Number of sentences per chunk
    #   chunk_overlap: [1, 2]  # Overlap in sentences
    #   add_file_name: en
    
    # # Third chunking method: Custom sentence window with custom splitter
    # - module_type: llama_index_chunk
    #   chunk_method: [SentenceWindow]
    #   sentence_splitter: custom_splitter
    #   window_size: [2, 3, 5]
    #   add_file_name: en

# RAG pipeline configuration for grid search
node_lines:
  # Retrieval phase with different options
  - node_line_name: retrieve_line
    nodes:
      - node_type: retrieval
        strategy:
          # Only using non-semantic metrics that don't require OpenAI
          metrics: [retrieval_precision, retrieval_recall, retrieval_f1]
          speed_threshold: 60
        top_k: [2, 3]  # Reduced from [3, 5] to decrease context size
        modules:
          - module_type: vectordb
            vectordb: nomic_embed
          - module_type: vectordb
            vectordb: mxbai_embed
          # - module_type: vectordb
          #   vectordb: flagembed_large
          # - module_type: vectordb
          #   vectordb: flagembed_small
          # - module_type: bm25
          # - module_type: hybrid_rrf  # Reciprocal Rank Fusion
          #   rrf_k: [1, 3]  # Reduced from [1, 3, 5]
  
  # Generation phase with multiple LLM models
  - node_line_name: generate_line
    nodes:
      # Prompt formatting
      - node_type: prompt_maker
        strategy:
          # Changed from rouge to bleu to avoid semantic evaluation
          metrics:
            - metric_name: bleu  # BLEU is a lexical metric that doesn't need OpenAI
          speed_threshold: 30
        modules:
          - module_type: fstring
            prompt: 
              # Simplified prompts to reduce context size
              - "Question: {query}\nContext: {retrieved_contents}\nAnswer:"
      
      # Generation with different models
      - node_type: generator
        strategy:
          # Changed from rouge to bleu to avoid semantic evaluation
          metrics:
            - metric_name: bleu  # BLEU is a lexical metric that doesn't need OpenAI
            # - metric_name: meteor  # Another non-semantic metric
          speed_threshold: 120
        modules:
          # Llama 3 model configuration
          - module_type: llama_index_llm
            llm: openailike
            model: llama3:latest
            api_base: "http://localhost:11434/v1"
            api_key: "ollama-not-needed-but-required"
            temperature: [0.0, 0.3]  # Reduced from [0.0, 0.3, 0.7]
            max_tokens: 1000  # Reduced from 1500
            request_timeout: 180
            timeout: 180
          
          # Mistral model configuration
          - module_type: llama_index_llm
            llm: openailike
            model: mistral:latest
            api_base: "http://localhost:11434/v1"
            api_key: "ollama-not-needed-but-required"
            temperature: [0.0, 0.3]  # Reduced from [0.0, 0.3, 0.7]
            max_tokens: 1000  # Reduced from 1500
            request_timeout: 180
            timeout: 180