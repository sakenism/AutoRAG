# Define your Vector DB configurations with multiple embedding models
vectordb:
  - name: nomic_embed
    db_type: chroma
    client_type: persistent
    embedding_model:
      type: ollama
      model_name: nomic-embed-text
    collection_name: nomic_embed
    path: ${PROJECT_DIR}/resources/chroma
    embedding_batch: 20
  
chunking:
  modules:
    # First chunking method: Token-based with different sizes
    - module_type: llama_index_chunk
      chunk_method: [Token]
      chunk_size: [256]
      add_file_name: en
    
node_lines:
  # Retrieval phase with different options
  - node_line_name: retrieve_line
    nodes:
      - node_type: retrieval
        strategy:
          # Only using non-semantic metrics that don't require OpenAI
          metrics: [retrieval_precision, retrieval_recall, retrieval_f1]
          speed_threshold: 60
        top_k: [2, 3]  # Reduced from [3, 5] to decrease context size
        modules:
          - module_type: vectordb
            vectordb: nomic_embed
  # Generation phase with multiple LLM models
  - node_line_name: generate_line
    nodes:
      # Prompt formatting
      - node_type: prompt_maker
        strategy:
          # Changed from rouge to bleu to avoid semantic evaluation
          metrics:
            - metric_name: bleu  # BLEU is a lexical metric that doesn't need OpenAI
          speed_threshold: 30
        modules:
          - module_type: fstring
            prompt: 
              # Simplified prompts to reduce context size
              - "Question: {query}\nContext: {retrieved_contents}\nAnswer:"
      
      # Generation with different models
      - node_type: generator
        strategy:
          # Changed from rouge to bleu to avoid semantic evaluation
          metrics:
            - metric_name: bleu  # BLEU is a lexical metric that doesn't need OpenAI
            - metric_name: meteor  # Another non-semantic metric
          speed_threshold: 120
        modules:
          # Llama 3 model configuration
          - module_type: llama_index_llm
            llm: openailike
            model: llama3:latest
            api_base: "http://100.98.3.202:11434/v1"
            api_key: "ollama-not-needed-but-required"
            temperature: [0.0, 0.3]  # Reduced from [0.0, 0.3, 0.7]
            max_tokens: 1000  # Reduced from 1500
            request_timeout: 180
            timeout: 180
          
          # Mistral model configuration
          # - module_type: llama_inde
          
